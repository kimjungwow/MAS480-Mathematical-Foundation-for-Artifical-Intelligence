{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (KFold, cross_val_score, train_test_split, cross_validate)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import (Pipeline, make_pipeline)\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso,LogisticRegression, LassoCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
      "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
      "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
      "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
      "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
      "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
      "\n",
      "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity3pm  Pressure9am  \\\n",
      "0           W           44.0          W  ...        22.0       1007.7   \n",
      "1         WNW           44.0        NNW  ...        25.0       1010.6   \n",
      "2         WSW           46.0          W  ...        30.0       1007.6   \n",
      "3          NE           24.0         SE  ...        16.0       1017.6   \n",
      "4           W           41.0        ENE  ...        33.0       1010.8   \n",
      "\n",
      "   Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  RISK_MM  \\\n",
      "0       1007.1       8.0       NaN     16.9     21.8         No      0.0   \n",
      "1       1007.8       NaN       NaN     17.2     24.3         No      0.0   \n",
      "2       1008.7       NaN       2.0     21.0     23.2         No      0.0   \n",
      "3       1012.8       NaN       NaN     18.1     26.5         No      1.0   \n",
      "4       1006.0       7.0       8.0     17.8     29.7         No      0.2   \n",
      "\n",
      "   RainTomorrow  \n",
      "0            No  \n",
      "1            No  \n",
      "2            No  \n",
      "3            No  \n",
      "4            No  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "(142193, 24)\n",
      "Sunshine         67816\n",
      "Evaporation      60843\n",
      "Cloud3pm         57094\n",
      "Cloud9am         53657\n",
      "Pressure9am      14014\n",
      "Pressure3pm      13981\n",
      "WindDir9am       10013\n",
      "WindGustDir       9330\n",
      "WindGustSpeed     9270\n",
      "WindDir3pm        3778\n",
      "Humidity3pm       3610\n",
      "Temp3pm           2726\n",
      "WindSpeed3pm      2630\n",
      "Humidity9am       1774\n",
      "RainToday         1406\n",
      "Rainfall          1406\n",
      "WindSpeed9am      1348\n",
      "Temp9am            904\n",
      "MinTemp            637\n",
      "MaxTemp            322\n",
      "RainTomorrow         0\n",
      "dtype: int64\n",
      "(112925, 61)\n"
     ]
    }
   ],
   "source": [
    "# Dataset downloaded from https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
    "train = pd.read_csv('./weatherAUS.csv')\n",
    "\n",
    "# Dataset looks like this\n",
    "print(train.head(5))\n",
    "print(train.shape)\n",
    "\n",
    "# Target is whether it will rain tomorrow\n",
    "\n",
    "\n",
    "# Above link recommends to exclude column RISK_MM\n",
    "# Since I will predict rain tomorrow in Austrailia, any city in Austrailia would be fine. So I drop column Location\n",
    "# In this example, date is just index for samples. So I dropped column Date.\n",
    "train=train.drop(['Location','RISK_MM','Date'],axis=1)\n",
    "\n",
    "# There are too many samples having NaN in columns Sunshine, Evaporation, Cloud3pm, Cloud9pm.\n",
    "# So I dropped four columns to preserve as many columns without NaN as possible.\n",
    "print(train.isnull().sum().sort_values(ascending=False))\n",
    "train=train.drop(['Sunshine','Evaporation','Cloud3pm','Cloud9am'],axis=1)\n",
    "\n",
    "# Then I deleted samples having NaN in any column.\n",
    "for col in train.columns:\n",
    "    temp=train.shape[0]\n",
    "    train = train[pd.notnull(train[col])]\n",
    "    \n",
    "# Change type of RainToday from string to int\n",
    "train['RainTodayBool'] = train['RainToday'].map( {'No': 0, 'Yes':1} ).astype(int)\n",
    "\n",
    "# RainTomorrow is our target.\n",
    "y=train['RainTomorrow'].map({'No':0, 'Yes':1}).astype(int)\n",
    "train = train.drop(['RainTomorrow','RainToday'],axis=1)\n",
    "\n",
    "# Change information about direction properly\n",
    "train=pd.get_dummies(train,columns=['WindGustDir','WindDir9am','WindDir3pm'])\n",
    "\n",
    "# Since some columns have relatively huge values, scale all columns to [0,1] range.\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(train)\n",
    "train = pd.DataFrame(scaler.transform(train), index=train.index, columns=train.columns)\n",
    "\n",
    "# Save training set for later usage\n",
    "ready=train\n",
    "print(ready.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find proper max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth =  1 Time taken :  46.6064088344574\n",
      "Mean :  0.8443748290340446  | Max :  0.8541316092463024  | Min :  0.8337908438855929  | Std :  0.006443277139344468\n",
      "max_depth =  2 Time taken :  108.06675839424133\n",
      "Mean :  0.8469252641257705  | Max :  0.8630889125044279  | Min :  0.8320198352962012  | Std :  0.009421325282729232\n",
      "max_depth =  3 Time taken :  193.5566053390503\n",
      "Mean :  0.8479613629672512  | Max :  0.8662770102727595  | Min :  0.8339679447445321  | Std :  0.009591014223238216\n",
      "max_depth =  4 Time taken :  315.1361620426178\n",
      "Mean :  0.8499715557443455  | Max :  0.8681367339709529  | Min :  0.8360046046223324  | Std :  0.009876666623623097\n",
      "max_depth =  5 Time taken :  485.64620900154114\n",
      "Mean :  0.8495110731218173  | Max :  0.8696422245837762  | Min :  0.8362702559107412  | Std :  0.010319239625865995\n",
      "max_depth =  6 Time taken :  732.0262486934662\n",
      "Mean :  0.8496438862144409  | Max :  0.8727417640807651  | Min :  0.8340564951740016  | Std :  0.011245182528709251\n"
     ]
    }
   ],
   "source": [
    "## Select proper max_depth.\n",
    "## I used default learning rate 0.1 this time, but I will change it later.\n",
    "\n",
    "for i in range(1,7):\n",
    "    t0=time.time()\n",
    "    gbrt = GradientBoostingClassifier(random_state=0, max_depth=i)\n",
    "    scores = cross_val_score(gbrt, train, y, cv=10)\n",
    "    print('max_depth = ',i,'Time taken : ', time.time()-t0)\n",
    "    print('Mean : ',np.mean(scores),' | Max : ',np.max(scores), ' | Min : ',np.min(scores), ' | Std : ',np.std(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find proper min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select proper min_samples_split.\n",
    "## I used default learning rate 0.1 this time, but I will change it later.\n",
    "## I also used max_depth=4 because it maximizes cross_validation score.\n",
    "\n",
    "min_samples_splits=[2,4,8,16,32]\n",
    "for i in min_samples_splits:\n",
    "    t0=time.time()\n",
    "    gbrt = GradientBoostingClassifier(random_state=0, max_depth=4, min_samples_split=i)\n",
    "    scores = cross_val_score(gbrt, train, y, cv=10)\n",
    "    print('min_samples_split = ',i,'Time taken : ', time.time()-t0)\n",
    "    print('Mean : ',np.mean(scores),' | Max : ',np.max(scores), ' | Min : ',np.min(scores), ' | Std : ',np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find proper min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select proper min_samples_leaf.\n",
    "## I used default learning rate 0.1 this time, but I will change it later.\n",
    "## I also used max_depth=4, min_samples_split=8 because these maximize cross_validation score.\n",
    "\n",
    "min_samples_leaves=[1,2,3,4,5]\n",
    "for i in min_samples_leaves:\n",
    "    t0=time.time()\n",
    "    gbrt = GradientBoostingClassifier(random_state=0, max_depth=4, min_samples_split=8,min_samples_leaf=i)\n",
    "    scores = cross_val_score(gbrt, train, y, cv=10)\n",
    "    print('min_samples_leaf = ',i,'Time taken : ', time.time()-t0)\n",
    "    print('Mean : ',np.mean(scores),' | Max : ',np.max(scores), ' | Min : ',np.min(scores), ' | Std : ',np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find proper max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features =  sqrt Time taken :  56.50385499000549\n",
      "Mean :  0.8467923977011876  | Max :  0.8662770102727595  | Min :  0.832905339590897  | Std :  0.009545501647487839\n",
      "max_features =  log2 Time taken :  45.690943479537964\n",
      "Mean :  0.8451098571999326  | Max :  0.8629117959617428  | Min :  0.8328167891614274  | Std :  0.00889301742825981\n"
     ]
    }
   ],
   "source": [
    "## Select proper max_features.\n",
    "## I used default learning rate 0.1 this time, but I will change it later.\n",
    "## I also used max_depth=4, min_samples_split=8, min_samples_leaf=1 \n",
    "## because these maximize cross_validation score.\n",
    "\n",
    "max_features_num=['None','7','8','9','sqrt','log2','auto']\n",
    "for i in max_features_num:\n",
    "    t0=time.time()\n",
    "    gbrt = GradientBoostingClassifier(random_state=0, max_depth=4, min_samples_split=8,min_samples_leaf=1,max_features=i)\n",
    "    scores = cross_val_score(gbrt, train, y, cv=10)\n",
    "    print('max_features = ',i,'Time taken : ', time.time()-t0)\n",
    "    print('Mean : ',np.mean(scores),' | Max : ',np.max(scores), ' | Min : ',np.min(scores), ' | Std : ',np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample =  0.9 Time taken :  320.0865890979767\n",
      "Mean :  0.849519942280691  | Max :  0.8686680835990082  | Min :  0.8369786593464978  | Std :  0.00979312844167559\n"
     ]
    }
   ],
   "source": [
    "## Select proper subsample to decide whether to use stochastic gradient boosting.\n",
    "## I used default learning rate 0.1 this time, but I will change it later.\n",
    "## I also used max_depth=4, min_samples_split=8, min_samples_leaf=1, max_features='None' \n",
    "## because these maximize cross_validation score.\n",
    "\n",
    "subsample=[1,0.95,0.9]\n",
    "for i in subsample:\n",
    "    t0=time.time()\n",
    "    gbrt = GradientBoostingClassifier(random_state=0, max_depth=4, min_samples_split=8,\n",
    "                                      min_samples_leaf=1,subsample=i)\n",
    "    scores = cross_val_score(gbrt, train, y, cv=10)\n",
    "    print('subsample = ',i,'Time taken : ', time.time()-t0)\n",
    "    print('Mean : ',np.mean(scores),' | Max : ',np.max(scores), ' | Min : ',np.min(scores), ' | Std : ',np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate =  0.05 Time taken :  634.7777450084686\n",
      "Mean :  0.8494225030862761  | Max :  0.8676939426142402  | Min :  0.8362702559107412  | Std :  0.009618764510497075\n"
     ]
    }
   ],
   "source": [
    "## Select proper subsample to decide whether to use stochastic gradient boosting.\n",
    "## I used default learning rate 0.1 this time, but I will change it later.\n",
    "## I also used max_depth=4, min_samples_split=8, min_samples_leaf=1, max_features='None' \n",
    "## because these maximize cross_validation score.\n",
    "\n",
    "learning_rates=[0.05,0.08,0.1]\n",
    "estimators=[200,125,100]\n",
    "for i in range(len(learning_rates):\n",
    "    t0=time.time()\n",
    "    gbrt = GradientBoostingClassifier(random_state=0, max_depth=4, min_samples_split=8,\n",
    "                                      min_samples_leaf=1, learning_rate=learning_rates[i], n_estimators=estimators[i])\n",
    "    scores = cross_val_score(gbrt, train, y, cv=10)\n",
    "    print('learning_rate = ',i,'Time taken : ', time.time()-t0)\n",
    "    print('Mean : ',np.mean(scores),' | Max : ',np.max(scores), ' | Min : ',np.min(scores), ' | Std : ',np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate =  0.08 Time taken :  388.978951215744\n",
      "Mean :  0.8496350272533357  | Max :  0.8676053843428976  | Min :  0.8359160541928629  | Std :  0.009674743505402346\n"
     ]
    }
   ],
   "source": [
    "## Select proper subsample to decide whether to use stochastic gradient boosting.\n",
    "## I used default learning rate 0.1 this time, but I will change it later.\n",
    "## I also used max_depth=4, min_samples_split=8, min_samples_leaf=1, max_features='None' \n",
    "## because these maximize cross_validation score.\n",
    "\n",
    "learning_rates=[0.08]\n",
    "for i in learning_rates:\n",
    "    t0=time.time()\n",
    "    gbrt = GradientBoostingClassifier(random_state=0, max_depth=4, min_samples_split=8,\n",
    "                                      min_samples_leaf=1, learning_rate=i, n_estimators=125)\n",
    "    scores = cross_val_score(gbrt, train, y, cv=10)\n",
    "    print('learning_rate = ',i,'Time taken : ', time.time()-t0)\n",
    "    print('Mean : ',np.mean(scores),' | Max : ',np.max(scores), ' | Min : ',np.min(scores), ' | Std : ',np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Time taken :  8.855034351348877\n",
      "Mean :  0.8467394235042788  | Max :  0.8672511512575275  | Min :  0.817851766581068  | Std :  0.013450573858038795\n",
      "Decision Tree Time taken :  3.3853774070739746\n",
      "Mean :  0.8381937769367948  | Max :  0.853600212558675  | Min :  0.8207739307535642  | Std :  0.010725376318694215\n",
      "Random Forest Time taken :  1.6386339664459229\n",
      "Mean :  0.7784458715479505  | Max :  0.7784961473740147  | Min :  0.7784272051009564  | Std :  1.891673118456956e-05\n",
      "AdaBoost Time taken :  58.5219464302063\n",
      "Mean :  0.8431263511068219  | Max :  0.8633545873184555  | Min :  0.8223678384840166  | Std :  0.01203407899902826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "ready=train\n",
    "t0=time.time()\n",
    "models=[]\n",
    "models_name=[]\n",
    "def create_models(alg,name):\n",
    "    models.append(alg)\n",
    "    models_name.append(name)\n",
    "create_models(LogisticRegression(random_state=0,max_iter=1000,solver=\"liblinear\"),'Logistic Regression')    \n",
    "create_models(DecisionTreeClassifier(random_state=0,max_depth=4),'Decision Tree')\n",
    "create_models(RandomForestClassifier(random_state=0,max_depth=4,n_estimators=10,max_features=1),'Random Forest')\n",
    "create_models(AdaBoostClassifier(),'AdaBoost')\n",
    "\n",
    "for i in range(len(models)):\n",
    "    t0=time.time()\n",
    "    scores=cross_val_score(models[i],train,y,cv=10)\n",
    "    print(models_name[i],'Time taken : ', time.time()-t0)\n",
    "    print('Mean : ',np.mean(scores),' | Max : ',np.max(scores), ' | Min : ',np.min(scores), ' | Std : ',np.std(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
